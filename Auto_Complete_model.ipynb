{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84740,"sourceType":"datasetVersion","datasetId":46601}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T13:46:56.559487Z","iopub.execute_input":"2024-08-08T13:46:56.559855Z","iopub.status.idle":"2024-08-08T13:46:57.674043Z","shell.execute_reply.started":"2024-08-08T13:46:56.559827Z","shell.execute_reply":"2024-08-08T13:46:57.672742Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/wikipedia-sentences/wikisent2.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:46:57.676095Z","iopub.execute_input":"2024-08-08T13:46:57.677366Z","iopub.status.idle":"2024-08-08T13:46:57.683803Z","shell.execute_reply.started":"2024-08-08T13:46:57.677328Z","shell.execute_reply":"2024-08-08T13:46:57.682697Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/wikipedia-sentences/wikisent2.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n\n# Function to read a text file and return its content\ndef read_file(file_path):\n    with open(file_path, 'r') as file:\n        content = file.readlines()\n    return content\n\n# Function to shuffle the lines in a file and save to a new file\ndef shuffle_and_sample(file_path, sample_size=1000, output_file='shuffled_sample.txt'):\n    # Read the original file\n    lines = read_file(file_path)\n    \n    # Shuffle the lines\n    random.shuffle(lines)\n    \n    # Take a sample of the shuffled lines\n    sample_lines = lines[:sample_size]\n    \n    # Save the sample to a new file\n    with open(output_file, 'w') as file:\n        file.writelines(sample_lines)\n\n# File path of the original text file\ninput_file_path = '/kaggle/input/wikipedia-sentences/wikisent2.txt'\n\n# Call the function to shuffle and sample\nshuffle_and_sample(input_file_path, sample_size=1000, output_file='shuffled_sample.txt')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:46:57.685365Z","iopub.execute_input":"2024-08-08T13:46:57.685822Z","iopub.status.idle":"2024-08-08T13:47:17.464675Z","shell.execute_reply.started":"2024-08-08T13:46:57.685787Z","shell.execute_reply":"2024-08-08T13:47:17.463464Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!ls\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:47:17.467631Z","iopub.execute_input":"2024-08-08T13:47:17.468295Z","iopub.status.idle":"2024-08-08T13:47:18.527880Z","shell.execute_reply.started":"2024-08-08T13:47:17.468260Z","shell.execute_reply":"2024-08-08T13:47:18.526765Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"shuffled_sample.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# Load the GPT-2 tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:47:18.529441Z","iopub.execute_input":"2024-08-08T13:47:18.529774Z","iopub.status.idle":"2024-08-08T13:47:45.431359Z","shell.execute_reply.started":"2024-08-08T13:47:18.529746Z","shell.execute_reply":"2024-08-08T13:47:45.430225Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-08-08 13:47:26.024330: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-08 13:47:26.024458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-08 13:47:26.176865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60050ae40904f98851e4a6243493800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de8c03558159480fad7d34bee5f50955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80370f3ad82b4320823b745538aeef31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67075758564249f891a5539a26784be7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01fc3817f88340a1b9385477bf8d5159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63764130acee4f4d9f766f130ee47ca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f49de1f662d544179c8861b6b11a64ed"}},"metadata":{}}]},{"cell_type":"code","source":"def load_dataset(file_path, tokenizer):\n    return TextDataset(\n        tokenizer=tokenizer,\n        file_path=file_path,\n        block_size=128\n    )\n\ntrain_dataset = load_dataset('shuffled_sample.txt', tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:47:45.433222Z","iopub.execute_input":"2024-08-08T13:47:45.434113Z","iopub.status.idle":"2024-08-08T13:47:45.880947Z","shell.execute_reply.started":"2024-08-08T13:47:45.434074Z","shell.execute_reply":"2024-08-08T13:47:45.879840Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:47:45.882330Z","iopub.execute_input":"2024-08-08T13:47:45.882672Z","iopub.status.idle":"2024-08-08T13:47:45.887511Z","shell.execute_reply.started":"2024-08-08T13:47:45.882643Z","shell.execute_reply":"2024-08-08T13:47:45.886325Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n    report_to=\"none\"  # Add this line to disable W&B\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:47:45.888859Z","iopub.execute_input":"2024-08-08T13:47:45.889218Z","iopub.status.idle":"2024-08-08T13:47:45.918871Z","shell.execute_reply.started":"2024-08-08T13:47:45.889172Z","shell.execute_reply":"2024-08-08T13:47:45.917625Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:47:45.920358Z","iopub.execute_input":"2024-08-08T13:47:45.920751Z","iopub.status.idle":"2024-08-08T13:59:00.558872Z","shell.execute_reply.started":"2024-08-08T13:47:45.920721Z","shell.execute_reply":"2024-08-08T13:59:00.557853Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [159/159 11:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=159, training_loss=3.6479979701012186, metrics={'train_runtime': 673.5619, 'train_samples_per_second': 0.944, 'train_steps_per_second': 0.236, 'total_flos': 41545433088000.0, 'train_loss': 3.6479979701012186, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model('./gpt2-autocomplete')\ntokenizer.save_pretrained('./gpt2-autocomplete')","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:00.561924Z","iopub.execute_input":"2024-08-08T13:59:00.562311Z","iopub.status.idle":"2024-08-08T13:59:01.314577Z","shell.execute_reply.started":"2024-08-08T13:59:00.562281Z","shell.execute_reply":"2024-08-08T13:59:01.313503Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('./gpt2-autocomplete/tokenizer_config.json',\n './gpt2-autocomplete/special_tokens_map.json',\n './gpt2-autocomplete/vocab.json',\n './gpt2-autocomplete/merges.txt',\n './gpt2-autocomplete/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"!ls results\n!ls results/runs","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:01.315899Z","iopub.execute_input":"2024-08-08T13:59:01.316233Z","iopub.status.idle":"2024-08-08T13:59:03.567077Z","shell.execute_reply.started":"2024-08-08T13:59:01.316207Z","shell.execute_reply":"2024-08-08T13:59:03.565661Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"checkpoint-159\nls: cannot access 'results/runs': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n\n# Define the path to your fine-tuned model and tokenizer\nmodel_dir = \"./gpt2-autocomplete\"\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(model_dir)\ntokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n\n# Create a text generation pipeline\ntext_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n\n# Generate text\nprompt = \"Your input text here\"\ngenerated_text = text_generator(prompt, max_length=50, truncation=True)\nprint(generated_text[0]['generated_text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:03.568957Z","iopub.execute_input":"2024-08-08T13:59:03.569351Z","iopub.status.idle":"2024-08-08T13:59:05.949420Z","shell.execute_reply.started":"2024-08-08T13:59:03.569318Z","shell.execute_reply":"2024-08-08T13:59:05.948294Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Your input text here is for reference only.\nThe list could be expanded or expanded by changing the name of the list editor to the editor or using JavaScript.\nIn November 1969, the International Institute for Strategic Studies (IBS) was founded to\n","output_type":"stream"}]},{"cell_type":"code","source":"prompts = [\n    \"The future of artificial intelligence is\",\n    \"In the year 2100, humanity will\",\n    \"The key to solving climate change lies in\"\n]\n\nfor prompt in prompts:\n    generated_text = text_generator(prompt, max_length=50, truncation=True)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Generated Text: {generated_text[0]['generated_text']}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:05.951411Z","iopub.execute_input":"2024-08-08T13:59:05.951739Z","iopub.status.idle":"2024-08-08T13:59:10.819301Z","shell.execute_reply.started":"2024-08-08T13:59:05.951713Z","shell.execute_reply":"2024-08-08T13:59:10.818086Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: The future of artificial intelligence is\nGenerated Text: The future of artificial intelligence is at a crossroads when it comes to predicting future behavior that will have serious impacts on the health and safety of humans.\nThe study presents five major features in the theory of how new devices of the future use human-\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: In the year 2100, humanity will\nGenerated Text: In the year 2100, humanity will have established an empire.\nIn 2015, a total of 7 billion people in the world were living on 3.5 billion acres of land.\nThe novel features characters from across both genders with the characters' journeys\n\nPrompt: The key to solving climate change lies in\nGenerated Text: The key to solving climate change lies in a transition to low-carbon energy sources, such as solar and wind power.\nThe team used a special type of carbon nanotubes developed at the University of Alabama to create a metal-reinforced\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Define the path to your fine-tuned model and tokenizer\nmodel_dir = \"./gpt2-autocomplete\"\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(model_dir)\ntokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n\n# Create a text generation pipeline\ntext_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n\n# Function to generate next word predictions\ndef predict_next_word(text, top_k=5):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    predictions = outputs.logits[:, -1, :]\n    top_k_predictions = predictions.topk(top_k).indices[0].tolist()\n    \n    suggested_words = [tokenizer.decode([pred]) for pred in top_k_predictions]\n    return suggested_words\n\n# Example usage\ntext = \"The future of artificial intelligence is\"\nsuggested_words = predict_next_word(text)\nprint(\"Next word suggestions:\", suggested_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:10.821112Z","iopub.execute_input":"2024-08-08T13:59:10.821577Z","iopub.status.idle":"2024-08-08T13:59:11.441208Z","shell.execute_reply.started":"2024-08-08T13:59:10.821537Z","shell.execute_reply":"2024-08-08T13:59:11.439868Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Next word suggestions: [' in', ' a', ' at', ' uncertain', ' still']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\n#Create a text generation pipline\ntest_generator = pipeline(\"text-generation\", model=model,tokenizer=tokenizer)\n\n# Function to generate next sentence\ndef predict_next_sentence(text, max_length=50):\n    generated_text = text_generator(text, max_length=max_length, num_return_sequences=1)\n    full_text = generated_text[0]['generated_text']\n    \n    # Split the generated text into sentences and return the first complete sentence\n    sentences = full_text.split('.')\n    if len(sentences) > 1:\n        return sentences[0] + '.'\n    else:\n        return full_text\n    \n#Ecample usage\ntext = 'In the year 2100, humanity will'\nnext_sentence = predict_next_sentence(text)\nprint(\"Next sentence suggestion:\", next_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:11.442604Z","iopub.execute_input":"2024-08-08T13:59:11.443038Z","iopub.status.idle":"2024-08-08T13:59:13.046326Z","shell.execute_reply.started":"2024-08-08T13:59:11.443010Z","shell.execute_reply":"2024-08-08T13:59:13.045237Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Next sentence suggestion: In the year 2100, humanity will go to a peak of 300 million live births, and 10 million people will live on Earth, with 8.\n","output_type":"stream"}]},{"cell_type":"code","source":"def writing_assistant():\n    print('Welcome to the writing Assisitant')\n    print(\"Start typing your text. type 'exit' to quit.\")\n    \n    context = \"\"\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            break\n            \n        context += \" \" + user_input\n        word_suggestions = predict_next_word(context)\n        sentence_suggestion = predict_next_sentence(context)\n        \n        print(\"Word suggestion:\", word_suggestions)\n        print(\"Sentences suggestion:\", sentence_suggestion)\n        \nwriting_assistant()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:59:13.047699Z","iopub.execute_input":"2024-08-08T13:59:13.048032Z","iopub.status.idle":"2024-08-08T14:15:52.201367Z","shell.execute_reply.started":"2024-08-08T13:59:13.048004Z","shell.execute_reply":"2024-08-08T14:15:52.199408Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Welcome to the writing Assisitant\nStart typing your text. type 'exit' to quit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  people like to say hi,\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Word suggestion: [' but', ' and', ' or', ' so', ' they']\nSentences suggestion:  people like to say hi, and I had no business in Canada after the 2008 financial crisis, so I played around with that a lot before taking a job.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  but i really don't like that\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Word suggestion: ['.', ' kind', ' word', ' term', ' name']\nSentences suggestion:  people like to say hi, but i really don't like that name.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:   they like to say hi. so\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Word suggestion: [' i', ',', ' when', ' it', ' what']\nSentences suggestion:  people like to say hi, but i really don't like that  they like to say hi.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  so \n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Word suggestion: ['ive', '\\xa0', 'ike', 'iced', 'lly']\nSentences suggestion:  people like to say hi, but i really don't like that  they like to say hi.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  so what do people like to say\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Word suggestion: [' hi', '?', ' to', ' when', ',']\nSentences suggestion:  people like to say hi, but i really don't like that  they like to say hi.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}